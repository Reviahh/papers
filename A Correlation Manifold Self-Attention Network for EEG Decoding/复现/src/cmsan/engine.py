"""
CMSAN Engine: Unified Training Core
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import time
import platform
import ctypes
from functools import partial, reduce
from typing import Dict, Any, Tuple, Optional, Callable, NamedTuple

import jax
import jax.numpy as jnp
from jax import random, lax
import equinox as eqx
import optax

# å†…éƒ¨å¯¼å…¥
from .model import CMSAN, batch_forward, batch_predict


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 0. ç±»å‹å®šä¹‰
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TrainState(NamedTuple):
    """ä¸å¯å˜è®­ç»ƒçŠ¶æ€"""
    model: CMSAN
    opt_state: optax.OptState
    key: jax.Array
    step: int


class TrainResult(NamedTuple):
    """è®­ç»ƒç»“æœ"""
    model: CMSAN
    train_acc: float
    test_acc: float
    loss_history: jax.Array
    duration: float
    params_count: int


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. å·¥å…·å‡½æ•°
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_core_info() -> str:
    """è·å–å½“å‰ CPU æ ¸å¿ƒ ID (Windows)"""
    if platform.system() == 'Windows':
        try:
            return f"#{ctypes.windll.kernel32.GetCurrentProcessorNumber()}"
        except:
            pass
    return "?"


def count_params(model: CMSAN) -> int:
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡"""
    return sum(x.size for x in jax.tree_util.tree_leaves(eqx.filter(model, eqx.is_array)))


def create_optimizer(
    lr: float,
    total_steps: int,
    weight_decay: float = 0.01,
    grad_clip: float = 1.0,
    warmup_ratio: float = 0.1,
) -> optax.GradientTransformation:
    """
    åˆ›å»ºä¼˜åŒ–å™¨ (AdamW + Cosine Decay + Warmup)
    """
    warmup_steps = int(total_steps * warmup_ratio)
    
    schedule = optax.warmup_cosine_decay_schedule(
        init_value=0.0,
        peak_value=lr,
        warmup_steps=warmup_steps,
        decay_steps=total_steps,
        end_value=lr * 0.01,
    )
    
    return optax.chain(
        optax.clip_by_global_norm(grad_clip),
        optax.adamw(schedule, weight_decay=weight_decay),
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. æ ¸å¿ƒè®¡ç®—å‡½æ•° (çº¯å‡½æ•°)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def compute_loss(model: CMSAN, xs: jax.Array, ys: jax.Array) -> jax.Array:
    """è®¡ç®—æ‰¹é‡äº¤å‰ç†µæŸå¤±"""
    logits = batch_forward(model, xs)
    return jnp.mean(optax.softmax_cross_entropy_with_integer_labels(logits, ys))


@eqx.filter_jit
def evaluate(model: CMSAN, xs: jax.Array, ys: jax.Array) -> jax.Array:
    """è®¡ç®—å‡†ç¡®ç‡"""
    preds = batch_predict(model, xs)
    return jnp.mean(preds == ys)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. SCAN æ¨¡å¼ (TPU/GPU å…¨å›¾ç¼–è¯‘)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _make_scan_trainer(
    optimizer: optax.GradientTransformation,
    batch_size: int,
    n_epochs: int,
    log_interval: int = 10,
) -> Callable:
    
    @eqx.filter_jit
    def train_scan(
        model: CMSAN,
        opt_state: optax.OptState,
        key: jax.Array,
        X: jax.Array,
        y: jax.Array,
        start_ts: float,
    ) -> Tuple[CMSAN, optax.OptState, jax.Array]:
        """å…¨å›¾ç¼–è¯‘è®­ç»ƒ"""
        
        N = X.shape[0]
        n_batches = N // batch_size
        
        # æ•°æ®è§„æ•´
        X_trimmed = X[:n_batches * batch_size]
        y_trimmed = y[:n_batches * batch_size]
        X_batched = X_trimmed.reshape(n_batches, batch_size, *X.shape[1:])
        y_batched = y_trimmed.reshape(n_batches, batch_size)
        
        def epoch_step(state, epoch_idx):
            m, o, k = state
            k, subk = random.split(k)
            perm = random.permutation(subk, n_batches)
            
            def batch_step(carry, batch_data):
                curr_m, curr_o = carry
                bx, by = batch_data
                
                loss, grads = eqx.filter_value_and_grad(compute_loss)(curr_m, bx, by)
                updates, new_o = optimizer.update(
                    grads, curr_o, eqx.filter(curr_m, eqx.is_array)
                )
                new_m = eqx.apply_updates(curr_m, updates)
                
                return (new_m, new_o), loss
            
            # æ‰“ä¹±æ‰¹æ¬¡é¡ºåº
            X_shuffled = jnp.take(X_batched, perm, axis=0)
            y_shuffled = jnp.take(y_batched, perm, axis=0)
            
            (m, o), losses = lax.scan(batch_step, (m, o), (X_shuffled, y_shuffled))
            avg_loss = jnp.mean(losses)
            
            # æ¡ä»¶æ—¥å¿—å›è°ƒ
            def log_callback(args):
                ep, loss, ts = args
                elapsed = time.time() - float(ts)
                print(f"Ep {int(ep)+1:<4} | {elapsed:>6.1f}s | Loss: {loss:.4f}")
            
            lax.cond(
                (epoch_idx + 1) % log_interval == 0,
                lambda _: jax.debug.callback(log_callback, (epoch_idx, avg_loss, start_ts)),
                lambda _: None,
                operand=None,
            )
            
            return (m, o, k), avg_loss
        
        (final_m, final_o, _), loss_history = lax.scan(
            epoch_step,
            (model, opt_state, key),
            jnp.arange(n_epochs),
        )
        
        return final_m, final_o, loss_history
    
    return train_scan


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. REDUCE æ¨¡å¼ (Windows æ··åˆæ¨¡å¼)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def _make_reduce_trainer(
    optimizer: optax.GradientTransformation,
    batch_size: int,
    n_epochs: int,
    log_interval: int = 10,
    verbose: bool = True,
) -> Callable:
    
    def train_reduce(
        model: CMSAN,
        opt_state: optax.OptState,
        key: jax.Array,
        X: jax.Array,
        y: jax.Array,
    ) -> Tuple[CMSAN, optax.OptState, jax.Array]:
        """æ··åˆæ¨¡å¼è®­ç»ƒ"""
        
        N = X.shape[0]
        n_batches = N // batch_size
        X_batched = X[:n_batches * batch_size].reshape(n_batches, batch_size, *X.shape[1:])
        y_batched = y[:n_batches * batch_size].reshape(n_batches, batch_size)
        
        start_time = time.time()
        loss_history = []
        
        # JIT ç¼–è¯‘çš„å• epoch å‡½æ•°
        @eqx.filter_jit
        def run_epoch(carry, perm):
            m, o = carry
            
            def batch_step(s, idx):
                curr_m, curr_o = s
                bx = jnp.take(X_batched, idx, axis=0)
                by = jnp.take(y_batched, idx, axis=0)
                
                loss, grads = eqx.filter_value_and_grad(compute_loss)(curr_m, bx, by)
                updates, new_o = optimizer.update(
                    grads, curr_o, eqx.filter(curr_m, eqx.is_array)
                )
                return (eqx.apply_updates(curr_m, updates), new_o), loss
            
            return lax.scan(batch_step, (m, o), perm)
        
        # Reduce è°ƒåº¦
        def epoch_step(accum, epoch_idx):
            curr_m, curr_o, curr_k = accum
            new_k, subkey = random.split(curr_k)
            perm = random.permutation(subkey, n_batches)
            
            (new_m, new_o), batch_losses = run_epoch((curr_m, curr_o), perm)
            loss_val = float(jnp.mean(batch_losses))
            loss_history.append(loss_val)
            
            # æ—¥å¿—
            if verbose and (epoch_idx + 1) % log_interval == 0:
                elapsed = time.time() - start_time
                print(f"Ep {epoch_idx+1:<4}/{n_epochs} | {elapsed:>6.1f}s | "
                      f"Core {get_core_info():<5} | Loss: {loss_val:.4f}")
            
            return (new_m, new_o, new_k)
        
        final_m, final_o, _ = reduce(
            epoch_step,
            range(n_epochs),
            (model, opt_state, key),
        )
        
        return final_m, final_o, jnp.array(loss_history)
    
    return train_reduce


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 5. ç»Ÿä¸€è®­ç»ƒæ¥å£
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def train_session(
    X_train: jax.Array,
    y_train: jax.Array,
    config: Dict[str, Any],
    key: jax.Array,
    X_test: Optional[jax.Array] = None,
    y_test: Optional[jax.Array] = None,
) -> TrainResult:
    """
    ç»Ÿä¸€è®­ç»ƒå…¥å£
    """
    start_time = time.time()
    
    # è§£åŒ…é…ç½®
    # æ³¨æ„: config å¯èƒ½å·²ç»æ˜¯æ‰å¹³åŒ–çš„ï¼Œæˆ–è€…åŒ…å«å­å­—å…¸
    # æˆ‘ä»¬çš„ configs/__init__.py ç°åœ¨è¿”å›çš„æ˜¯åµŒå¥—ç»“æ„:
    # { 'train': {...}, 'model': { 'D':20, 'C':22... } }
    
    # æå–è®­ç»ƒå‚æ•° (ä¼˜å…ˆä» train å­—æ®µå–ï¼Œå¦‚æœæ²¡æœ‰å°±ä»æ ¹ç›®å½•å–)
    train_cfg = config.get('train', config)
    # æå–æ¨¡å‹å‚æ•°
    model_cfg = config.get('model', {})
    
    # è®­ç»ƒè¶…å‚
    epochs = train_cfg.get('epochs', 100)
    batch_size = train_cfg.get('batch_size', 64)
    lr = train_cfg.get('lr', 1e-3)
    verbose = train_cfg.get('verbose', True)
    log_interval = train_cfg.get('log_interval', 10)
    engine_mode = train_cfg.get('engine', 'auto')
    weight_decay = train_cfg.get('weight_decay', 0.01)
    grad_clip = train_cfg.get('grad_clip', 1.0)
    
    # æ•°æ®ä¿¡æ¯
    N = X_train.shape[0]
    
    # åˆ†å‰²å¯†é’¥
    k_model, k_train = random.split(key)
    
    # -------------------------------------------------------------------------
    # 1. åˆ›å»ºæ¨¡å‹ (Fixed)
    # -------------------------------------------------------------------------
    # ç›´æ¥ä½¿ç”¨ model_cfg é‡Œçš„å‚æ•°ï¼Œå®ƒç°åœ¨åº”è¯¥åŒ…å« C, T, K, D, S ç­‰æ‰€æœ‰å¿…è¦ä¿¡æ¯
    try:
        model = CMSAN(
            key=k_model,
            **model_cfg 
        )
    except TypeError as e:
        print("\nâŒ Model Init Error: Maybe config is missing 'C', 'T', or 'K'?")
        print(f"Current model_cfg keys: {list(model_cfg.keys())}")
        raise e
    
    params_count = count_params(model)
    
    if verbose:
        print(f"ğŸ§  Model: {params_count:,} params")
        print(f"ğŸ“Š Data: N={N}, C={model.C}, T={model.T}")
    
    # 2. åˆ›å»ºä¼˜åŒ–å™¨
    steps_per_epoch = N // batch_size
    total_steps = epochs * steps_per_epoch
    
    optimizer = create_optimizer(
        lr=lr,
        total_steps=total_steps,
        weight_decay=weight_decay,
        grad_clip=grad_clip,
    )
    opt_state = optimizer.init(eqx.filter(model, eqx.is_array))
    
    # 3. é€‰æ‹©å¼•æ“
    use_scan = (
        engine_mode == 'scan' or
        (engine_mode == 'auto' and platform.system() != 'Windows' and not verbose)
    )
    
    if verbose:
        engine_name = 'SCAN (Whole-Graph)' if use_scan else 'REDUCE (Hybrid)'
        print(f"ğŸ”§ Engine: {engine_name}")
        print(f"ğŸš€ Training: {epochs} epochs, batch={batch_size}, lr={lr}")
        print("-" * 60)
    
    # 4. è®­ç»ƒ
    if use_scan:
        trainer = _make_scan_trainer(optimizer, batch_size, epochs, log_interval)
        model, _, loss_history = trainer(
            model, opt_state, k_train, X_train, y_train, time.time()
        )
    else:
        trainer = _make_reduce_trainer(optimizer, batch_size, epochs, log_interval, verbose)
        model, _, loss_history = trainer(model, opt_state, k_train, X_train, y_train)
    
    # ç¡®ä¿è®¡ç®—å®Œæˆ
    jax.block_until_ready(eqx.filter(model, eqx.is_array))
    
    # 5. è¯„ä¼°
    train_acc = float(evaluate(model, X_train, y_train))
    
    if X_test is not None and y_test is not None:
        test_acc = float(evaluate(model, X_test, y_test))
    else:
        test_acc = 0.0
    
    duration = time.time() - start_time
    
    if verbose:
        print("-" * 60)
        print(f"âœ… Done in {duration:.1f}s")
        print(f"ğŸ“ Train Acc: {train_acc:.2%}")
        if X_test is not None:
            print(f"ğŸ† Test Acc:  {test_acc:.2%}")
    
    return TrainResult(
        model=model,
        train_acc=train_acc,
        test_acc=test_acc,
        loss_history=loss_history,
        duration=duration,
        params_count=params_count,
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 6. æ£€æŸ¥ç‚¹ç®¡ç†
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def save_checkpoint(model: CMSAN, path: str) -> None:
    """ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹"""
    os.makedirs(os.path.dirname(path) or '.', exist_ok=True)
    with open(path, 'wb') as f:
        eqx.tree_serialise_leaves(f, model)
    print(f"ğŸ’¾ Saved: {path}")


def load_checkpoint(path: str, model_template: CMSAN) -> CMSAN:
    """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹ (éœ€è¦æ¨¡å‹æ¨¡æ¿)"""
    with open(path, 'rb') as f:
        return eqx.tree_deserialise_leaves(f, model_template)


# å…¼å®¹æ—§æ¥å£
fit_unified = train_session
evaluate_pure = evaluate