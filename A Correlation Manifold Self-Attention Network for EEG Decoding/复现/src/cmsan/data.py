"""
CMSAN Generic Data Adapter (Fixed)
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ä¿®å¤å†…å®¹:
1. è¡¥å› DATASET_META å˜é‡ï¼Œè§£å†³ ImportErrorã€‚
2. ä¸“æ³¨äº .mat æ–‡ä»¶å¤„ç†ã€‚
"""

import logging
import glob
from pathlib import Path
from typing import Tuple, Optional, List, Dict

import numpy as np
import jax.numpy as jnp
from scipy.io import loadmat

logger = logging.getLogger(__name__)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. å…ƒæ•°æ®å®šä¹‰ (ä¿ç•™æ­¤å˜é‡ä»¥å…¼å®¹ __init__.py å¯¼å…¥)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# å³ä½¿æˆ‘ä»¬åšé€šç”¨åŠ è½½ï¼Œä¿ç•™è¿™ä¸ªå­—å…¸ä¹Ÿæœ‰åŠ©äºå¿«é€Ÿå®šä½å·²çŸ¥æ•°æ®é›†çš„æ–‡ä»¶å¤¹
DATASET_META = {
    'bcic':       {'folder': 'BCICIV_2a_mat'},
    'bciciv_2a':  {'folder': 'BCICIV_2a_mat'},
    'mamem':      {'folder': 'MAMEM'},
    'bcicha':     {'folder': 'BCIcha'},
}

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. é€šç”¨å·¥å…·
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def normalize(X: np.ndarray) -> np.ndarray:
    """é€šç”¨ Z-Score æ ‡å‡†åŒ–"""
    axes = tuple(range(1, X.ndim))
    mean = X.mean(axis=axes, keepdims=True)
    std = X.std(axis=axes, keepdims=True) + 1e-8
    return (X - mean) / std

def find_dataset_dir(base_name: str, root_dir: Path) -> Path:
    """æ¨¡ç³ŠæŸ¥æ‰¾æ•°æ®é›†æ–‡ä»¶å¤¹"""
    # 1. ç²¾ç¡®åŒ¹é…
    target = root_dir / base_name
    if target.exists(): return target
    
    # 2. æŸ¥è¡¨ (Meta)
    lower_name = base_name.lower()
    if lower_name in DATASET_META:
        folder = DATASET_META[lower_name]['folder']
        target = root_dir / folder
        if target.exists(): return target

    # 3. æ¨¡ç³ŠåŒ¹é… (å¿½ç•¥å¤§å°å†™/ä¸‹åˆ’çº¿)
    clean_name = lower_name.replace('_', '').replace('-', '')
    for d in root_dir.iterdir():
        if not d.is_dir(): continue
        d_clean = d.name.lower().replace('_', '').replace('-', '')
        if clean_name in d_clean or d_clean in clean_name:
            return d
            
    # æ‰¾ä¸åˆ°å°±è¿”å› rootï¼Œå‡è®¾æ–‡ä»¶åœ¨æ ¹ç›®å½•
    return root_dir

def smart_extract_mat(data_dict: dict) -> Tuple[np.ndarray, np.ndarray]:
    """
    æ™ºèƒ½æå– .mat å†…å®¹
    é€»è¾‘ï¼šæœ€å¤§çš„æ•°ç»„æ˜¯ Xï¼Œç¬¬äºŒå¤§(æˆ–åå­—å«label)çš„æ˜¯ y
    """
    candidates = []
    # è¿‡æ»¤æ‰ __header__, __version__ ç­‰
    for k, v in data_dict.items():
        if k.startswith('__'): continue
        if isinstance(v, np.ndarray) and v.size > 1:
            candidates.append((k, v))
            
    if len(candidates) < 2:
        # å¦‚æœåªæœ‰ä¸€ä¸ªæ•°ç»„ï¼Œæ‰“å°å‡ºæ¥çœ‹çœ‹
        keys = list(data_dict.keys())
        raise ValueError(f"Mat file needs at least 2 arrays (Data & Label). Found: {keys}")
        
    # æŒ‰å­—èŠ‚å¤§å°æ’åºï¼Œæœ€å¤§çš„é€šå¸¸æ˜¯ EEG æ•°æ®
    candidates.sort(key=lambda x: x[1].nbytes, reverse=True)
    
    # 1. ç¡®å®š X (æœ€å¤§çš„)
    X_key, X = candidates[0]
    
    # 2. ç¡®å®š y
    y = None
    # ä¼˜å…ˆæ‰¾åå­—åƒæ ‡ç­¾çš„
    for k, v in candidates[1:]:
        name = k.lower()
        if any(tag in name for tag in ['y', 'label', 'class', 'target', 'truth']):
            y = v
            break
            
    # å¦‚æœæ²¡æ‰¾åˆ°æ˜¾å¼åå­—ï¼Œå°±é»˜è®¤å–ç¬¬äºŒå¤§çš„æ•°ç»„
    if y is None:
        y = candidates[1][1]
        
    logger.info(f"   ğŸ”§ Smart Extract: X='{X_key}' {X.shape}, y found.")
    return X, y

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. æ ¸å¿ƒåŠ è½½é€»è¾‘
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def ensure_shape(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
    """
    é€šç”¨å½¢çŠ¶ä¿®æ­£
    ç›®æ ‡: X -> (Batch, Channel, Time), y -> (Batch,)
    """
    # 1. ä¿®æ­£ y (Flatten)
    y = y.flatten()
    # è‡ªåŠ¨ä¿®æ­£ 1-based indexing (Matlab ä¹ æƒ¯)
    if y.min() == 1:
        y -= 1
        
    # 2. ä¿®æ­£ X
    # å¦‚æœæ˜¯ 3D (N, A, B)
    if X.ndim == 3:
        N, A, B = X.shape
        # å¯å‘å¼è½¬ç½®ï¼šé€šå¸¸ Time(T) > Channel(C)
        # å¦‚æœç¬¬2ç»´æ¯”ç¬¬3ç»´å¤§å¾ˆå¤š (ä¾‹å¦‚ A=1000, B=22)ï¼Œé‚£ A å¯èƒ½æ˜¯æ—¶é—´
        # æˆ‘ä»¬éœ€è¦ (N, C, T) -> (N, Short, Long)
        if A > B and A > 50:
            logger.info(f"   âš ï¸ Auto-Transpose: (N, T, C) {X.shape} -> (N, C, T)")
            X = np.swapaxes(X, 1, 2)
            
    # å¦‚æœæ˜¯ 2D (N, T)ï¼Œæ‰©å……ä¸º (N, 1, T)
    elif X.ndim == 2:
        X = X[:, np.newaxis, :]

    return X, y

def load_unified(dataset_name: str, subject_id: int) -> Tuple[jnp.ndarray, jnp.ndarray]:
    """
    ç»Ÿä¸€å…¥å£
    """
    # 1. å®šä½æ•°æ®æ ¹ç›®å½•
    base_dir = Path(__file__).parent.parent.parent / "data"
    if not base_dir.exists():
        base_dir = Path("data")
        
    # 2. æŸ¥æ‰¾ç›®å½•
    data_dir = find_dataset_dir(dataset_name, base_dir)
    logger.info(f"   ğŸ“‚ Searching in: {data_dir.name}")

    # 3. æŸ¥æ‰¾è¢«è¯•æ–‡ä»¶ (.mat)
    # åŒ¹é…æ¨¡å¼: *1.mat*, *01*.mat
    patterns = [
        f"*{subject_id}.mat",
        f"*{subject_id:02d}*.mat",
        f"*{subject_id}*.mat", # å®½æ³›åŒ¹é…
    ]
    
    found_file = None
    all_mat_files = list(data_dir.glob("*.mat"))
    
    # æ‰«æ
    for pat in patterns:
        matches = list(data_dir.glob(pat))
        if matches:
            # æ‰¾åˆ°æœ€å¤§çš„é‚£ä¸ªæ–‡ä»¶ï¼ˆé˜²æ­¢åŒ¹é…åˆ°åªæœ‰ header çš„å°æ–‡ä»¶ï¼‰
            matches.sort(key=lambda f: f.stat().st_size, reverse=True)
            found_file = matches[0]
            break
            
    if not found_file:
        raise FileNotFoundError(f"No .mat file found for Subject {subject_id} in {data_dir}")

    logger.info(f"   ğŸ“„ Loading: {found_file.name}")

    # 4. åŠ è½½ .mat
    try:
        mat_data = loadmat(str(found_file))
        X, y = smart_extract_mat(mat_data)
    except Exception as e:
        raise RuntimeError(f"Failed to load {found_file.name}: {e}")

    # 5. åå¤„ç†
    X, y = ensure_shape(X, y)
    X = normalize(X)
    
    return jnp.array(X, dtype=jnp.float32), jnp.array(y, dtype=jnp.int32)