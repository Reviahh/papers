# CorAtt 框架解析：意义、接口与架构

## 一、复现的意义是什么？

### 论文提出的不是"一个模型"，而是"一套理论框架"

```
┌─────────────────────────────────────────────────────────────────────────┐
│                     CorAtt 的三层贡献                                    │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 第一层：数学理论                                                  │   │
│  │                                                                   │   │
│  │   "如何在相关矩阵流形上做深度学习？"                               │   │
│  │                                                                   │   │
│  │   • 相关矩阵构成李群 (Corr⁺⁺, ⊙)                                 │   │
│  │   • 定义了两种黎曼度量: OLM 和 LSM                                │   │
│  │   • 推导了李群同态作为变换层                                       │   │
│  │   • 推导了加权Fréchet均值作为聚合操作                             │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              ↓                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 第二层：算法框架                                                  │   │
│  │                                                                   │   │
│  │   "流形上的注意力机制通用模板"                                     │   │
│  │                                                                   │   │
│  │   Q,K,V = hom(X)           # 李群同态生成                         │   │
│  │   A = softmax(f(dist(Q,K))) # 测地距离计算相似度                   │   │
│  │   R = WFM(A, V)            # Fréchet均值聚合                      │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                              ↓                                          │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │ 第三层：具体应用                                                  │   │
│  │                                                                   │   │
│  │   EEG解码只是一个实例！                                           │   │
│  │                                                                   │   │
│  │   可扩展到：                                                       │   │
│  │   • 金融时序（股票相关性分析）                                     │   │
│  │   • 脑连接组学（fMRI功能连接）                                     │   │
│  │   • 推荐系统（用户-物品相关性）                                    │   │
│  │   • 任何需要建模"变量间关系"的场景                                 │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 为什么要用相关矩阵而不是协方差矩阵？

```
协方差矩阵 P:                    相关矩阵 C:
┌─────────────────┐              ┌─────────────────┐
│ 受尺度影响       │              │ 尺度不变        │
│                 │              │                 │
│ 电极A: 0-100μV  │  ≠           │ 归一化到[-1,1]  │
│ 电极B: 0-10μV   │              │ 只关心"关系"    │
│                 │              │                 │
│ → 数值差异大    │              │ → 数值可比较    │
└─────────────────┘              └─────────────────┘

EEG场景：不同电极的信号强度差异很大，但我们关心的是
        "哪些脑区在协同工作" → 相关性！
```

---

## 二、框架全景图

```
┌──────────────────────────────────────────────────────────────────────────────────────┐
│                                    CorAtt 框架全景                                    │
├──────────────────────────────────────────────────────────────────────────────────────┤
│                                                                                      │
│   ┌─────────────┐                                                                    │
│   │ 原始信号     │     EEG / 金融时序 / 任意多变量信号                                │
│   │ x ∈ ℝ^{C×T} │     C=通道/变量数, T=时间步                                        │
│   └──────┬──────┘                                                                    │
│          │                                                                           │
│          ▼                                                                           │
│   ┌──────────────────────────────────────────────────────────────────┐              │
│   │              特征提取模块 (FEM) - 可替换                          │              │
│   │  ┌─────────────────────────────────────────────────────────┐    │              │
│   │  │  spatial_conv: ℝ^{C×T} → ℝ^{D×T}   (跨通道混合)         │    │              │
│   │  │  temporal_conv: ℝ^{D×T} → ℝ^{D×T}  (时间特征提取)       │    │              │
│   │  └─────────────────────────────────────────────────────────┘    │              │
│   │  接口: h = FEM(x, θ_fem)                                        │              │
│   └──────────────────────────────────────────────────────────────────┘              │
│          │                                                                           │
│          ▼                                                                           │
│   ┌──────────────────────────────────────────────────────────────────┐              │
│   │              流形建模模块 (MMM) - 核心创新                        │              │
│   │  ┌─────────────────────────────────────────────────────────┐    │              │
│   │  │  split: ℝ^{D×T} → [ℝ^{D×T/S}]_S    (时间分段)           │    │              │
│   │  │  corr:  ℝ^{D×T/S} → Corr⁺⁺_D       (计算相关矩阵)       │    │              │
│   │  └─────────────────────────────────────────────────────────┘    │              │
│   │  接口: Cs = [corr(seg) for seg in split(h)]                     │              │
│   │  输出: S 个 D×D 相关矩阵                                         │              │
│   └──────────────────────────────────────────────────────────────────┘              │
│          │                                                                           │
│          ▼                                                                           │
│   ╔══════════════════════════════════════════════════════════════════╗              │
│   ║              相关矩阵注意力 (CorAtt) - 核心贡献                   ║              │
│   ║  ┌─────────────────────────────────────────────────────────┐    ║              │
│   ║  │  1. 李群同态生成 Q,K,V                                   │    ║              │
│   ║  │     Q = hom(C, θ_q)                                      │    ║              │
│   ║  │     K = hom(C, θ_k)                                      │    ║              │
│   ║  │     V = hom(C, θ_v)                                      │    ║              │
│   ║  │                                                          │    ║              │
│   ║  │  2. 测地距离计算注意力                                    │    ║              │
│   ║  │     s_ij = 1/(1 + log(1 + dist(Q_i, K_j)))               │    ║              │
│   ║  │     A = softmax(s)                                       │    ║              │
│   ║  │                                                          │    ║              │
│   ║  │  3. 加权Fréchet均值聚合                                   │    ║              │
│   ║  │     R_i = WFM(A_i, V)                                    │    ║              │
│   ║  └─────────────────────────────────────────────────────────┘    ║              │
│   ║  接口: Rs = attention(Cs, θ_att)                                ║              │
│   ╚══════════════════════════════════════════════════════════════════╝              │
│          │                                                                           │
│          ▼                                                                           │
│   ┌──────────────────────────────────────────────────────────────────┐              │
│   │              切空间投影 (TangentMap) - 流形→欧氏                  │              │
│   │  ┌─────────────────────────────────────────────────────────┐    │              │
│   │  │  logo:     Corr⁺⁺ → Hol(n)   (黎曼对数映射)             │    │              │
│   │  │  vec_tril: Hol(n) → ℝ^{n(n-1)/2}  (向量化下三角)        │    │              │
│   │  │  concat:   [ℝ^d]_S → ℝ^{S·d}  (拼接)                    │    │              │
│   │  └─────────────────────────────────────────────────────────┘    │              │
│   │  接口: features = concat([vec_tril(logo(R)) for R in Rs])       │              │
│   └──────────────────────────────────────────────────────────────────┘              │
│          │                                                                           │
│          ▼                                                                           │
│   ┌──────────────────────────────────────────────────────────────────┐              │
│   │              分类器 (Classifier) - 可替换                        │              │
│   │  ┌─────────────────────────────────────────────────────────┐    │              │
│   │  │  FC + Softmax: ℝ^{S·d} → ℝ^K → Δ^{K-1}                  │    │              │
│   │  └─────────────────────────────────────────────────────────┘    │              │
│   │  接口: logits = W @ features + b                                │              │
│   └──────────────────────────────────────────────────────────────────┘              │
│          │                                                                           │
│          ▼                                                                           │
│   ┌─────────────┐                                                                    │
│   │ 预测结果     │                                                                    │
│   │ ŷ ∈ {1..K}  │                                                                    │
│   └─────────────┘                                                                    │
│                                                                                      │
└──────────────────────────────────────────────────────────────────────────────────────┘
```

---

## 三、核心接口详解

### 1. 流形几何层 (`manifold.py`)

```python
"""
这一层定义了"相关矩阵流形上的基本运算"
相当于定义了一个新的"几何空间"的规则
"""

# ═══════════════════════════════════════════════════════════════
#                        接口一览
# ═══════════════════════════════════════════════════════════════

corr(X)           # 数据 → 相关矩阵      (进入流形)
logo(C)           # 相关矩阵 → 李代数    (流形 → 切空间)
expo(S)           # 李代数 → 相关矩阵    (切空间 → 流形)
dist(C1, C2)      # 测地距离             (流形上的距离)
wfm(ws, Cs)       # 加权Fréchet均值      (流形上的平均)
cayley(A)         # 参数 → 正交矩阵      (约束优化)
vec_tril(X)       # 对称矩阵 → 向量      (离开流形)
```

```
数据空间 ℝ^{D×T}          相关矩阵流形 Corr⁺⁺           切空间 Hol(n) ≅ ℝ^{n(n-1)/2}
      │                        │                              │
      │    corr(X)             │                              │
      ├───────────────────────→│                              │
      │                        │       logo(C)                │
      │                        ├─────────────────────────────→│
      │                        │                              │
      │                        │       expo(S)                │
      │                        │←─────────────────────────────┤
      │                        │                              │
      │                        │   dist(C1, C2)               │
      │                        │◄────────────►│               │
      │                        │   (测地线长度)                │
      │                        │                              │
      │    vec_tril(logo(C))   │                              │
      │←───────────────────────┼──────────────────────────────┤
      │                        │                              │
```

### 2. 李群同态层 (`model.py: hom`)

```python
"""
这是论文的核心创新：保持李群结构的变换层

普通神经网络: y = Wx + b  (线性变换)
李群同态:     Y = hom(X)  (保结构变换)

关键性质: hom(X ⊙ Y) = hom(X) ⊙ hom(Y)
         变换后再乘 = 先乘再变换
"""

def hom(C, A):
    """
    李群同态: Corr⁺⁺_n → Corr⁺⁺_n
    
    输入:
        C: (D, D) 相关矩阵
        A: (D, D) 可学习参数 (会被转换为正交矩阵)
    
    输出:
        (D, D) 变换后的相关矩阵
    
    数学:
        M = Cayley(A)  ∈ O(n)  (正交矩阵)
        hom(C) = Expo(Off(Mᵀ Logo(C) M))
    
    几何意义:
        1. Logo(C): 把 C 投影到切空间 (李代数)
        2. Mᵀ·M:    在切空间中旋转/变换
        3. Off:     保持李代数的结构 (去对角)
        4. Expo:    映射回流形
    """
    M = cayley(A)          # 参数 → 正交矩阵
    L = logo(C)            # 流形 → 切空间
    L_transformed = M.T @ L @ M  # 切空间中变换
    return expo(off(L_transformed))  # 回到流形
```

```
                    李群同态的几何图像
    
    Corr⁺⁺ (流形)                    Hol(n) (切空间/李代数)
    
         C ●                              ○ Logo(C)
          \                              /|
           \  Expo                      / |
            \                          /  | Mᵀ(·)M
             \                        /   | (旋转)
              \                      /    ↓
               ●──────────────────→ ○ Off(Mᵀ Logo(C) M)
             hom(C)                     
    
    整个过程: 流形 → 切空间 → 变换 → 回到流形
    保持了李群的代数结构！
```

### 3. 注意力层 (`model.py: attention`)

```python
"""
相关矩阵流形上的自注意力

与标准 Transformer 的对比:

标准 Transformer:
    Q, K, V = Linear(X)
    Attention = softmax(QKᵀ/√d)
    Output = Attention @ V

CorAtt:
    Q, K, V = hom(C)           # 李群同态 (保结构)
    Attention = softmax(f(geodesic_dist(Q, K)))  # 测地距离
    Output = WFM(Attention, V)  # Fréchet均值 (流形上的平均)
"""

def attention(Cs, θ):
    """
    相关矩阵注意力
    
    输入:
        Cs: (S, D, D) S个相关矩阵
        θ:  参数字典，包含 θ['Aq'], θ['Ak'], θ['Av']
    
    输出:
        Rs: (S, D, D) S个精炼后的相关矩阵
    
    流程:
        1. 通过李群同态生成 Q, K, V
        2. 用测地距离计算注意力分数
        3. 用加权Fréchet均值聚合
    """
```

```
              注意力机制对比图
    
    ┌────────────────────────────────────────────────────────────┐
    │                   标准 Transformer                         │
    │                                                            │
    │    X ∈ ℝⁿ ──Linear──→ Q,K,V ∈ ℝⁿ                         │
    │                          │                                 │
    │                    QKᵀ (点积)                              │
    │                          │                                 │
    │                    softmax                                 │
    │                          │                                 │
    │                    Σ aᵢvᵢ (加权平均)                       │
    │                          │                                 │
    │                    Output ∈ ℝⁿ                            │
    └────────────────────────────────────────────────────────────┘
    
    ┌────────────────────────────────────────────────────────────┐
    │                      CorAtt                                │
    │                                                            │
    │    C ∈ Corr⁺⁺ ──hom──→ Q,K,V ∈ Corr⁺⁺                    │
    │                          │                                 │
    │              dist(Q,K) (测地距离)                          │
    │                          │                                 │
    │         1/(1+log(1+d)) + softmax                          │
    │                          │                                 │
    │              WFM(A,V) (Fréchet均值)                        │
    │                          │                                 │
    │                    Output ∈ Corr⁺⁺                        │
    └────────────────────────────────────────────────────────────┘
    
    关键区别:
    • 变换: Linear → 李群同态 (保结构)
    • 相似度: 点积 → 测地距离 (几何意义)
    • 聚合: 加权平均 → Fréchet均值 (流形上有效)
```

### 4. 切空间投影 (`model.py: tangent projection`)

```python
"""
从流形回到欧氏空间，用于分类

为什么需要这一步？
- 流形是弯曲的，不能直接用线性分类器
- 切空间是流形的"局部线性近似"
- 在切空间可以使用标准的全连接层
"""

def tangent_projection(Rs):
    """
    输入: Rs (S, D, D) 流形上的相关矩阵
    输出: features (S × D(D-1)/2,) 欧氏向量
    
    流程:
        1. logo(R): 投影到切空间 (去除曲率)
        2. vec_tril: 向量化 (矩阵→向量)
        3. concat: 拼接所有段的特征
    """
```

```
                切空间投影的几何意义
    
        流形 Corr⁺⁺                    切空间 T_I(Corr⁺⁺)
        (弯曲表面)                      (平面)
    
             ●  R₃                           
            /                                   ○ log(R₃)
           /                                   /
          ● R₂          logo              ○ log(R₂)
         /           ─────────→              /
        ● R₁                           ○ log(R₁)
                                             │
                                             │ vec_tril
                                             ↓
                                         [v₁, v₂, v₃]
                                             │
                                             │ concat
                                             ↓
                                        features ∈ ℝⁿ
                                             │
                                             │ FC
                                             ↓
                                         logits ∈ ℝᴷ
```

---

## 四、代码模块职责

```
┌─────────────────────────────────────────────────────────────────────────┐
│                          代码架构                                        │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   manifold.py                           model.py                        │
│   ┌─────────────────────┐               ┌─────────────────────┐        │
│   │ 纯数学运算           │               │ 神经网络层           │        │
│   │                     │               │                     │        │
│   │ • corr    数据→流形  │               │ • init      初始化   │        │
│   │ • logo    流形→切空间│   ────────→   │ • hom       同态层   │        │
│   │ • expo    切空间→流形│   (调用)      │ • attention 注意力   │        │
│   │ • dist    测地距离   │               │ • forward   前向传播 │        │
│   │ • wfm     Fréchet均值│               │                     │        │
│   │ • cayley  正交参数化 │               │                     │        │
│   │ • vec_tril 向量化    │               │                     │        │
│   │                     │               │                     │        │
│   │ 无可学习参数         │               │ 包含可学习参数 θ     │        │
│   └─────────────────────┘               └─────────────────────┘        │
│            │                                      │                     │
│            │                                      │                     │
│            ▼                                      ▼                     │
│   ┌─────────────────────────────────────────────────────────┐          │
│   │                      train.py                           │          │
│   │                                                         │          │
│   │   • loss_batch    计算损失                               │          │
│   │   • train_step    单步更新 (JIT编译)                     │          │
│   │   • train         完整训练循环                           │          │
│   │   • evaluate      评估准确率                             │          │
│   │   • save/load     模型持久化                             │          │
│   │                                                         │          │
│   └─────────────────────────────────────────────────────────┘          │
│                              │                                          │
│                              ▼                                          │
│   ┌─────────────────────────────────────────────────────────┐          │
│   │                      optim.py                           │          │
│   │                                                         │          │
│   │   • init_adam     初始化优化器状态                        │          │
│   │   • adam          Adam更新                               │          │
│   │   • sgd_momentum  SGD+动量                               │          │
│   │   • cosine_decay  学习率调度                             │          │
│   │                                                         │          │
│   └─────────────────────────────────────────────────────────┘          │
│                              │                                          │
│                              ▼                                          │
│   ┌─────────────────────────────────────────────────────────┐          │
│   │                      main.py                            │          │
│   │                                                         │          │
│   │   • 配置管理 (CFG_BCIC, CFG_LIGHT)                       │          │
│   │   • 数据加载                                             │          │
│   │   • 训练入口                                             │          │
│   │   • 命令行接口                                           │          │
│   │                                                         │          │
│   └─────────────────────────────────────────────────────────┘          │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

---

## 五、扩展接口：如何用这个框架？

### 场景1: 换一个任务 (比如金融时序)

```python
# 只需要改配置和数据，框架不变！

CFG_FINANCE = {
    'C': 50,      # 50只股票
    'T': 240,     # 一年的日数据
    'D': 30,      # 特征维度
    'S': 4,       # 按季度分段
    'K': 3,       # 涨/跌/平
}

# 数据: (N, 50, 240) 股票收益率
# 相关矩阵自动捕捉股票间的联动关系
```

### 场景2: 换一个度量 (LSM 替代 OLM)

```python
# 在 manifold.py 中实现 LSM 操作

def log_star(C):
    """LSM: Corr → Row0(n)"""
    # ... 阻尼牛顿法
    pass

def exp_star(S):
    """LSM: Row0(n) → Corr"""
    # ...
    pass

# 然后在 model.py 中切换
def hom_lsm(C, A):
    M = cayley(A)
    L = log_star(C)
    # ...
```

### 场景3: 换一个特征提取器

```python
# FEM 是可插拔的！

def fem_transformer(x, θ):
    """用 Transformer 替代卷积"""
    # ...
    return h

def fem_lstm(x, θ):
    """用 LSTM 替代卷积"""
    # ...
    return h

# 只要输出 (D, T) 的特征，后面的流形部分不变
```

### 场景4: 多头注意力

```python
def multi_head_attention(Cs, θ, num_heads=4):
    """
    多头相关矩阵注意力
    """
    heads = []
    for h in range(num_heads):
        Rs_h = attention(Cs, θ[f'head_{h}'])
        heads.append(Rs_h)
    
    # 合并多头 (比如取 Fréchet 均值)
    return merge_heads(heads)
```

---

## 六、一图总结

```
┌──────────────────────────────────────────────────────────────────────────┐
│                        CorAtt = 流形上的 Transformer                      │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│     Transformer                          CorAtt                          │
│     ───────────                          ──────                          │
│                                                                          │
│     数据空间: ℝⁿ                         数据空间: Corr⁺⁺ (流形)          │
│                                                                          │
│     变换层:   Linear                     变换层:   李群同态               │
│               y = Wx + b                           Y = Expo(M'Logo(X)M)  │
│                                                                          │
│     相似度:   点积                       相似度:   测地距离               │
│               qᵀk                                  ‖Logo(Q)-Logo(K)‖     │
│                                                                          │
│     聚合:     加权平均                   聚合:     Fréchet均值            │
│               Σwᵢvᵢ                                Expo(Σwᵢ Logo(Vᵢ))    │
│                                                                          │
│     输出:     ℝⁿ                         输出:     Corr⁺⁺                 │
│                                                                          │
├──────────────────────────────────────────────────────────────────────────┤
│                                                                          │
│     为什么这么设计？                                                      │
│                                                                          │
│     1. 相关矩阵不在欧氏空间，普通运算会破坏其结构                          │
│     2. 李群同态保持代数结构 → 输出仍是有效的相关矩阵                       │
│     3. 测地距离是流形上的"真实距离" → 相似度计算更准确                    │
│     4. Fréchet均值是流形上的"几何中心" → 聚合结果仍在流形上                │
│                                                                          │
│     → 整个计算过程都尊重数据的内在几何！                                   │
│                                                                          │
└──────────────────────────────────────────────────────────────────────────┘
```

需要我补充哪个部分的细节？比如：
- LSM 度量的完整实现？
- 多头注意力版本？
- 具体数据集的加载代码？